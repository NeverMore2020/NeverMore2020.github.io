---
layout: post
title: PCA主成分分析法
date: 2021-03-01
categories: blog
tags: [Algorithm]
description: 文章金句。
---
最近在学习数据降维的时候再次碰到了PCA，之前也遇到多次但理解总是不到位，索性做一下记录吧！(大三上学期Advanced Transform 中也学过，当时本菜鸡不知道在说啥。。)

# 什么是主成分分析法

**主成分分析法**：英文全名 Principal Component Analysis 简称 PCA ，由名字就可以看出来，这是一个挑重点分析的方法。主成分分析 法是通过 恰当 的数学变换 ，使新变量—— 主成分成为原变量 的线性 组合 ，并选 取少数 几个在变差总信息量中 比例较 大的主成分来分析 事物 的一种方法 。 主成分在变差信息量中的比例越大 ， 它在综合评价 中的作用就越大

**思想**：整体思想就是化繁为简，抓住问题关键，也就是降维思想。当然，既然是抓住关键，那么自然就是以牺牲精度为代价。

**解决问题**：因为每个变量都在不同程度上反映了所研究问题的某些信息，并且指标之间彼此有一定的相关性，因而所得的统计数据反映的信息在一定程度上有重叠。在用统计方法研究多变量问题时，变量太多会增加计算量和分析问题的复杂性。

人们希望在进行定量分析过程中，涉及的变量较少，得到的信息量较多。为了尽可能的减少冗余和噪音，一般情况可以从相关变量中选择一个，或者把几个相关变量综合为一个变量作为代表，用少数变量来代表所有变量。

**原理**：因为评估所涉及的众多变量之间既然有一定的相关性，就必然存在着起支配作用的因素。根据这一点，通过对原始变量和相关矩阵的内部结构的关系研究，找出影响目标变量某一要素的几个综合指标，使综合指标为原来变量的线性拟合。这样，综合指标不仅保留了原始变量的主要信息，且彼此间不相关，又比原始变量具有某些更优越的性质，使得我们在研究复杂目标变量评估问题时，容易抓住主要矛盾。

**形象理解**

比如，某学籍数据，有两列 M 和 F ，其中M 列的取值是如果学生为男性，则取值为 1 如果为女性，则取值为 0 。F 列，如果为男性则取值为 0 否则取值为一。由这两种关系可以知道，这两列数据是强相关的。只要保留一列，就能够完全还原另外一列。 当然，不要局限于数据删除，还有数据转换，删除可以理解为在此方法中的一种方式。

当然，上述情况在真实数据中是不可能出现的。这里只是借此介绍一下这种思维。真实情况中，我们需要考虑删除哪一列信息可以使得损失最小？或者是通过变换数据就能使得损失信息更小？又如何度量信息的丢失量？原始数据的处理降维有哪些步骤？

坐标示例：

我们来看下面这张图，这是一个椭圆的点阵。椭圆上面有一个长轴和一个短轴。现在我们要表示点阵的主要变化趋势，就可以以长短轴（或者平行于长短轴）构建新的坐标系。在极端的情况下，短轴变成了一个点，那么长轴就能代表这个点阵的趋势和特点。这样，一个二维数据，就变成了一维。

![pca1](https://user-images.githubusercontent.com/61499991/111863282-95b91800-8995-11eb-8bc3-413e8f29a9dc.jpg)



# 基础知识储备

## 内积与投影：

内积运算，将两个向量映射为一个实数。其几何意义就是 向量 A ，在向量 B 的投影长度。（下图是以二维向量为例，多维空间依然是如此。）


![pca2](https://user-images.githubusercontent.com/61499991/111863310-df096780-8995-11eb-9c14-ce828d11d941.jpg)
![pca3](https://user-images.githubusercontent.com/61499991/111863322-f21c3780-8995-11eb-968d-dbfcb84e8bdd.jpg)


上式中，B 为单位向量

## 基：

同样以上图 B为例，B向量为（3，2）其表示的其实为在 X 轴的投影值为3 ，在Y轴的投影值 为 2 。这其实加入了一个隐含信息，就是本坐标轴 分别是以 X Y轴为方向的单位向量。这里的 X Y 轴其实就是我们所提到的 基。只不过一般默认为 （1，0）和（0，1）

所以呢，要描述一组向量，首先是要确定一组基。然后求这个向量在这组基中的投影即可。对基的要求是线性无关，并不一定非要正交。但是因为正交基有较好的性质，所以一般情况我们都是用正交基。

## 基变换

上面我们了解了基的原理。如果同样把（3，2）放到新基里面描述，那就是把向量和新基相乘即可。

如果是在描述中，有多个基呢？那就是与基阵相乘。



## 协方差：

在概率论与统计学中，协方差用于衡量两个随机变量的联合变化程度。而方差则是协方差的一种特殊情况，即变量与自身的协方差。

期望：在概率论和统计学中，一个离散性随机变量的期望值（或数学期望，亦简称期望，物理学中称为期待值）是试验中每次可能的结果乘以其结果概率的总和。比如骰子的期望值为 1* 1/6 +2*1/6 + …+ 6*1/6 = 3.5

协方差公式为：
![pca4](https://user-images.githubusercontent.com/61499991/111863362-1ed04f00-8996-11eb-8b7e-20d54ef17a1c.jpg)


其中，E(X) = u E(Y) = v

协方差表示的是两个变量的总体的误差，这与只表示一个变量误差的方差不同。 如果两个变量的变化趋势一致，也就是说如果其中一个大于自身的期望值，另外一个也大于自身的期望值，那么两个变量之间的协方差就是正值。 如果两个变量的变化趋势相反，即其中一个大于自身的期望值，另外一个却小于自身的期望值，那么两个变量之间的协方差就是负值。如果X 与Y 是统计独立的，那么二者之间的协方差就是0

# 流程和步骤

## 第一步：标准化

把输入数据集变量的范围标准化，以使它们中的每一个均可以大致成比例的分析。简单说，就是要把存在较大差异的数据转变为可比较的数据。比如把 0-100 的变量转化为 0-1 的变量。这一步一般可以通过减去平均值，再除以每个变量值的标准差来完成。




## 第二步：协方差矩阵计算

这一步的目的是：了解输入数据集的变量是如何相对于平均值变化的。或者换句话说，是为了查看它们之间是否存在任何关系。因为有时候，变量间高度相关是因为它们包含大量的信息。因此，为了识别这些相关性，我们进行协方差矩阵计算。

协方差矩阵是p×p对称矩阵（其中p是维数），其所有可能的初始变量与相关联的协方差作为条目。

好了，现在我们知道协方差矩阵只不过是一个表，汇总了所有可能配对的变量间相关性。下面就是计算协方差矩阵的特征向量和特征值，以筛选主要成分。

## 第三步：计算协方差矩阵的特征向量和特征值，用以识别主成分

特征向量和特征值都是线性代数概念，需要从协方差矩阵计算得出，以便确定数据的主成分。开始解释这些概念之前，让我们首先理解主成分的含义

主成分是由初始变量的线性组合或混合构成的新变量。该组合中新变量（如主成分）之间彼此不相关，且大部分初始变量都被压缩进首个成分中。所以，10维数据会显示10个主成分，但是PCA试图在第一个成分中得到尽可能多的信息，然后在第二个成分中得到尽可能多的剩余信息，以此类推。

例如，假设你有一个10维数据，你最终将得到的内容如下面的屏幕图所示，其中第一个主成分包含原始数据集的大部分信息，而最后一个主成分只包含其中的很少部分。因此，以这种方式组织信息，可以在不丢失太多信息的情况下减少维度，而这需要丢弃携带较少信息的成分。

![pca5](https://user-images.githubusercontent.com/61499991/111863849-d6666080-8998-11eb-914d-d489fc2336f2.jpg)

在这里，方差和信息间的关系是，线所承载的方差越大，数据点沿着它的分散也越大，沿着线的散点越多，它所携带的信息也越多。简单地说，只要把主成分看作是提供最佳角度来观察和评估数据的新轴，这样观测结果之间的差异就会更明显。

协方差矩阵的特征向量实际上是方差最多的轴的方向（或最多的信息），我们称之为主成分。通过特征值的顺序对特征向量进行排序，从最高到最低，你就得到了按重要性排序的主成分。

## 第四步：特征向量

正如我们在上一步中所看到的，计算特征向量并按其特征值依降序排列，使我们能够按重要性顺序找到主成分。在这个步骤中我们要做的，是选择保留所有成分还是丢弃那些重要性较低的成分（低特征值），并与其他成分形成一个向量矩阵，我们称之为特征向量。

因此，特征向量只是一个矩阵，其中包含我们决定保留的成分的特征向量作为列。这是降维的第一步，因为如果我们选择只保留n个特征向量（分量）中的p个，则最终数据集将只有p维。

## 第五步：沿主成分轴重新绘制数据

在前面的步骤中，除了标准化之外，你不需要更改任何数据，只需选择主成分，形成特征向量，但输入数据集时要始终与原始轴统一（即初始变量）。

这一步，也是最后一步，目标是使用协方差矩阵的特征向量去形成新特征向量，将数据从原始轴重新定位到由主成分轴中（因此称为主成分分析）。这可以通过将原始数据集的转置乘以特征向量的转置来完成。

# 祭上KLT！！！
![pca6](https://user-images.githubusercontent.com/61499991/111864035-a7042380-8999-11eb-9b96-2f36f8aa490a.jpg)
![pca7](https://user-images.githubusercontent.com/61499991/111864038-a8355080-8999-11eb-9135-b639f8e11416.jpg)


# 优缺点

优点：化繁为简，降低了计算量。

缺点：一定程度上损失了精度。并且只能处理“线性问题”，这是一种线性降维技术、

